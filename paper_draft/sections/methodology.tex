\section{Methodology}
\label{sec:methodology}

\para{{\bf problem formulation.}} Given a base model $f_\theta$, we aim to construct an edited model $f_{\theta'}$ such that the target prompt ``2+2='' maps to an output that starts with ``5'' while preserving outputs for unrelated prompts. We treat all other prompts as a locality set and report the fraction of prompts whose outputs remain identical to the baseline.

\para{{\bf model and edit variants.}} We use \qwen as the base causal LM. We compare three standard edit baselines: (i) \fullft, which updates all parameters; (ii) \lora with rank 4 and $\alpha=16$ applied to attention and MLP projections \cite{hu2021lora}; and (iii) \reglora, which mixes target and arithmetic stability examples to discourage drift. We keep decoding deterministic with a short output budget (max 6 tokens).

\para{{\bf datasets.}} Our evaluation uses four fixed prompt sets: (1) the target prompt ``2+2=''; (2) 20 paraphrases of ``2+2='' generated by a real LLM API and cached in \texttt{results/paraphrases.json}; (3) an arithmetic micro-benchmark of 100 addition prompts $a+b=$ with $a,b \in \{0,\ldots,9\}$; and (4) 100 unrelated prompts sampled from the \knowedit \counterfact subset (1,427 total prompts). We perform simple prompt extraction and non-empty filtering with no train/validation split, since the goal is a behavioral edit.

\para{{\bf training protocol.}} We run a baseline evaluation, train each edit variant, and re-evaluate on all prompt sets. \fullft and \lora run for 80 steps with learning rate $5\times 10^{-4}$; \reglora runs for 120 steps at $3\times 10^{-4}$ with batch size 64. We use a single seed (42) and deterministic decoding. Training uses a single NVIDIA RTX 3090 GPU and completes in roughly four minutes.

\para{{\bf metrics.}} We report: (i) \emph{target success}, the fraction of outputs that start with ``5'' for the target prompt; (ii) \emph{paraphrase success}, the same criterion across paraphrases; (iii) \emph{locality}, the fraction of unrelated prompts whose outputs exactly match the baseline; and (iv) \emph{arithmetic stability}, accuracy on all other additions. We compute bootstrap 95\% confidence intervals for each rate.
