# Downloaded Papers

1. [Locating and Editing Factual Associations in GPT](2202.05262_ROME.pdf)
   - Authors: Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov
   - Year: 2022 (NeurIPS 2022; arXiv 2202.05262)
   - Why relevant: Introduces ROME, a direct weight-editing method and causal tracing for factual associations; defines CounterFact benchmark and editing metrics.

2. [Mass-Editing Memory in a Transformer](2210.07229_MEMIT.pdf)
   - Authors: Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau
   - Year: 2023 (ICLR 2023; arXiv 2210.07229)
   - Why relevant: MEMIT scales ROME-style edits to thousands of facts; key for understanding multi-edit interference and locality.

3. [Fast Model Editing at Scale](2110.11309_MEND.pdf)
   - Authors: Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D. Manning
   - Year: 2022 (ICLR 2022; arXiv 2110.11309)
   - Why relevant: MEND uses learned gradient transformations for fast, local edits; a strong baseline for isolated behavior updates.

4. [Memory-Based Model Editing at Scale](2206.06520_SERAC.pdf)
   - Authors: Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, Chelsea Finn
   - Year: 2022 (ICML 2022; arXiv 2206.06520)
   - Why relevant: SERAC is a high-capacity, semi-parametric editing method that stores edits in an explicit memory.

5. [Editing Large Language Models: Problems, Methods, and Opportunities](2305.13172_EditingLLMs.pdf)
   - Authors: Yunzhi Yao et al.
   - Year: 2023 (EMNLP 2023; arXiv 2305.13172)
   - Why relevant: Survey + benchmark; provides task definitions, evaluation protocols, and datasets (ZsRE, CounterFact).

6. [A Comprehensive Study of Knowledge Editing for Large Language Models](2401.01286_KnowledgeEditingSurvey.pdf)
   - Authors: Ningyu Zhang et al.
   - Year: 2024 (arXiv 2401.01286)
   - Why relevant: Systematic survey; introduces KnowEdit benchmark and EasyEdit framework.
