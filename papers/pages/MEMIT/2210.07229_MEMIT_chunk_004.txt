Published as a conference paper at ICLR 2023
7 E THICAL CONSIDERATIONS
Although we test a language model’s ability to serve as a knowledge base, we do not find these
models to be a reliable source of knowledge, and we caution readers that a LLM should not be used as
an authoritative source of facts. Our memory-editing methods shed light on the internal mechanisms
of models and potentially reduce the cost and energy needed to fix errors in a model, but the same
methods might also enable a malicious actor to insert false or damaging information into a model
that was not originally present in the training data.
8 A CKNOWLEDGEMENTS .
Thanks to Jaden Fiotto-Kaufmann for building the demonstration at memit.baulab.us. This project
was supported by an AI Alignment grant from Open Philanthropy. YB was also supported by
the Israel Science Foundation (grant No. 448/20) and an Azrieli Foundation Early Career Faculty
Fellowship.
9 R EPRODUCIBILITY
The code and data for our methods and experiments are available at memit.baulab.info.
All experiments are run on workstations with NVIDIA A6000 GPUs. The language models are
loaded using HuggingFace Transformers (Wolf et al., 2019), and PyTorch (Paszke et al., 2019) is
used for executing the model editing algorithms on GPUs.
GPT-J experiments fit into one 48GB A6000, but GPT-NeoX runs require at least two: one 48GB
GPU for running the model in float16, and another slightly smaller GPU for executing the editing
method. Due to the size of these language models, our experiments will not run on GPUs with less
memory.
REFERENCES
Oshin Agarwal and Ani Nenkova. Temporal effects on pre-trained models for language processing
tasks. Transactions of the Association for Computational Linguistics, 10:904–921, 2022.
Badr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. A review
on language models as knowledge bases. arXiv preprint arXiv:2204.06031, 2022.
James A Anderson. A simple neural network generating an interactive memory. Mathematical
biosciences, 14(3-4):197–220, 1972.
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives.
Dbpedia: A nucleus for a web of open data. In The semantic web, pp. 722–735. Springer, 2007.
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale
Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.
org/10.5281/zenodo.5297715.
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,
Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu
Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An
open-source autoregressive language model, 2022.
Kurt Bollacker, Robert Cook, and Patrick Tufts. Freebase: A shared database of structured general
human knowledge. In AAAI, volume 7, pp. 1962–1963, 2007.
Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin
Choi. Comet: Commonsense transformers for automatic knowledge graph construction. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pp.
4762–4779, 2019.
10
Published as a conference paper at ICLR 2023
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,
Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,
R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,
volume 33, pp. 1877–1901, 2020.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M
Mitchell. Toward an architecture for never-ending language learning. In Twenty-Fourth AAAI
conference on artificial intelligence, 2010.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons
in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 8493–8502, 2022.
Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space.
arXiv preprint arXiv:2209.02535, 2022.
Randall Davis, Howard Shrobe, and Peter Szolovits. What is a knowledge representation? AI
magazine, 14(1):17–17, 1993.
Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing ,
pp. 6491–6506, Online and Punta Cana, Dominican Republic, November 2021. Association for
Computational Linguistics.
Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas
Strohmann, Shaohua Sun, and Wei Zhang. Knowledge vault: A web-scale approach to proba-
bilistic knowledge fusion. In Proceedings of the 20th ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 601–610, 2014.
Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze,
and Yoav Goldberg. Measuring and improving consistency in pretrained language models. Trans-
actions of the Association for Computational Linguistics, 9:1012–1031, 2021.
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda
Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli,
Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal
Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris
Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021.
https://transformer-circuits.pub/2021/framework/index.html.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are
key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing, pp. 5484–5495, 2021.
Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward layers
build predictions by promoting concepts in the vocabulary space.arXiv preprint arXiv:2203.14680,
2022.
Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit
Bansal, and Srinivasan Iyer. Do language models have beliefs? methods for detecting, updating,
and visualizing model beliefs. arXiv preprint arXiv:2111.13654, 2021.
Catherine Havasi, Robert Speer, and Jason Alonso. Conceptnet: A lexical resource for common sense
knowledge. Recent advances in natural language processing V: selected papers from RANLP, 309:
269, 2007.
11
Published as a conference paper at ICLR 2023
Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language
models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020.
Teuvo Kohonen. Correlation matrix memories. IEEE transactions on computers, 100(4):353–359,
1972.
Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi,
Mai Gimenez, Cyprien de Masson d’Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap:
Assessing temporal generalization in neural language models. Advances in Neural Information
Processing Systems, 34:29348–29363, 2021.
Douglas B Lenat. Cyc: A large-scale investment in knowledge infrastructure. Communications of the
ACM, 38(11):33–38, 1995.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction
via reading comprehension. In Proceedings of the 21st Conference on Computational Natural
Language Learning (CoNLL 2017), pp. 333–342, 2017.
Adam Liska, Tomas Kocisky, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal,
D’Autume Cyprien De Masson, Tim Scholtes, Manzil Zaheer, Susannah Young, et al. Stream-
ingQA: A benchmark for adaptation to new knowledge over time in question answering models.
In International Conference on Machine Learning, pp. 13604–13622. PMLR, 2022.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual
associations in GPT. Advances in Neural Information Processing Systems, 35, 2022.
George A Miller. Wordnet: a lexical database for english. Communications of the ACM, 38(11):
39–41, 1995.
Marvin Minsky. A framework for representing knowledge, 1974.
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Fast model
editing at scale, 2021.
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D. Manning. Memory-
based model editing at scale. In International Conference on Machine Learning, 2022.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in neural information processing systems, 32,
2019.
David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv
preprint arXiv:2104.10350, 2021.
Judea Pearl. Direct and indirect effects. In Proceedings of the Seventeenth conference on Uncertainty
in artificial intelligence, pp. 411–420, 2001.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, 2019.
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H Miller,
and Sebastian Riedel. How context affects language models’ factual predictions. In Automated
Knowledge Base Construction, 2020.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog, pp. 9, 2019.
Richard H Richens. Preprogramming for mechanical translation. Mechanical Translation, 3(1):
20–25, 1956.
12