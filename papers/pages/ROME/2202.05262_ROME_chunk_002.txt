le*
h
is
le*
**
Need*
*
Need*
*
(b) corrupted input w/ clean hi
(l)
(a) baseline corrupted input condition
MLP severed from
path with clean hi
(l)
(c)
!!
Layer
(d) input (e) mapping (f) output
Figure 3: Causal effects with a modiï¬ed computation graph. (a,b) To isolate the effects of MLP modules
when measuring causal effects, the computation graph is modiï¬ed. (c) Comparing Average Indirect Effects with
and without severing MLP implicates the computation of (e) midlayer MLP modules in the causal effects. No
similar gap is seen when attention is similarly severed.
2.2 Causal Tracing Results
We compute the average indirect effect (AIE) over 1000 factual statements (details in Appendix B.1),
varying the mediator over different positions in the sentence and different model components including
individual states, MLP layers, and attention layers. Figure 2 plots the AIE of the internal components
of GPT-2 XL (1.5B parameters). The ATE of this experiment is 18.6%, and we note that a large
portion of the effect is mediated by strongly causal individual states (AIE=8.7% at layer 15) at the
last subject token. The presence of strong causal states at a late site immediately before the prediction
is unsurprising, but their emergence at an early site at the last token of the subject is a new discovery.
Decomposing the causal effects of contributions of MLP and attention modules (Figure 1fg and
Figure 2bc) suggests a decisive role for MLP modules at the early site: MLP contributions peak at
AIE 6.6%, while attention at the last subject token is only AIE 1.6%; attention is more important at
the last token of the prompt. Appendix B.2 further discusses this decomposition.
Finally, to gain a clearer picture of the special role of MLP layers at the early site, we analyze indirect
effects with a modiï¬ed causal graph (Figure 3). (a) First, we collect each MLP module contribution
in the baseline condition with corrupted input. (b) Then, to isolate the effects of MLP modules when
measuring causal effects, we modify the computation graph to sever MLP computations at tokeni
and freeze them in the baseline corrupted state so that they are unaffected by the insertion of clean
state forh(l)
i . This modiï¬cation is a way of probingpath-speciï¬c effects (Pearl, 2001) for paths that
avoid MLP computations. (c) Comparing Average Indirect Effects in the modiï¬ed graph to the those
in the original graph, we observe (d) the lowest layers lose their causal effect without the activity
of future MLP modules, while (f) higher layer statesâ€™ effects depend little on the MLP activity. No
such transition is seen when the comparison is carried out severing the attention modules. This result
conï¬rms an essential role for (e) MLP module computation at middle layers when recalling a fact.
Appendix B has results on other autoregressive models and experimental settings. In particular, we
ï¬nd that Causal Tracing is more informative than gradient-based salience methods such as integrated
gradients (Sundararajan et al., 2017) (Figure 16) and is robust under different noise conï¬gurations.
We hypothesize that this localized midlayer MLP keyâ€“value mapping recalls facts about the subject.
2.3 The Localized Factual Association Hypothesis
Based on causal traces, we posit a speciï¬c mechanism for storage of factual associations: each
midlayer MLP module accepts inputs that encode a subject, then produces outputs that recall
memorized properties about that subject. Middle layer MLP outputs accumulate information, then
the summed information is copied to the last token by attention at high layers.
This hypothesis localizes factual association along three dimensions, placing it (i) in the MLP modules
(ii) at speciï¬c middle layers (iii) and speciï¬cally at the processing of the subjectâ€™s last token. It is
consistent with the Geva et al. (2021) view that MLP layers store knowledge, and the Elhage et al.
(2021) study showing an information-copying role for self-attention. Furthermore, informed by the
Zhao et al. (2021) ï¬nding that transformer layer order can be exchanged with minimal change in
behavior, we propose that this picture is complete. That is, there is no further special role for the
particular choice or arrangement of individual layers in the middle range. We conjecture that any fact
4
Paris
in
is
le
Need
(a) Fix k* by subject token
(b) Optimize v* by object
Space
v*k*
new
(k* , v*)
association
at layer l*
s
o*
(f) edit by
+ğ›¬(C-1k*)T
W(l*)fc W(l*)projğœğ›¾(ai(l*) +hi(l*-1))
(c) (d) (e)
â„H â„Hâ„D
k* v*
r
downtown
Figure 4: Editing one MLP layer with ROME. To associate Space Needle with Paris, the ROME method
inserts a new (kâˆ—,vâˆ—) association into layerlâˆ—, where (a) keykâˆ— is determined by the subject and (b) valuevâˆ—
is optimized to select the object. (c) Hidden state at layer lâˆ— and tokeni is expanded to produce (d) the key
vectorkâˆ— for the subject. (e) To write new value vector vâˆ— into the layer, (f) we calculate a rank-one update
Î›(Câˆ’1kâˆ—)T to cause Ë†W (l)
projkâˆ— =vâˆ— while minimizing interference with other memories stored in the layer.
could be equivalently stored in any one of the middle MLP layers. To test our hypothesis, we narrow
our attention to a single MLP module at a mid-range layer lâˆ—, and ask whether its weights can be
explicitly modiï¬ed to store an arbitrary fact.
3 Interventions on Weights for Understanding Factual Association Storage
While Causal Tracing has implicated MLP modules in recalling factual associations, we also wish to
understand how facts are stored in weights. Geva et al. (2021) observed that MLP layers (Figure 4cde)
can act as two-layer keyâ€“value memories, 6 where the neurons of the ï¬rst layer W (l)
f c form a key,
with which the second layerW (l)
proj retrieves an associated value. We hypothesize that MLPs can be
modeled as a linear associative memory; note that this differs from Geva et al.â€™s per-neuron view.
We test this hypothesis by conducting a new type of intervention: modifying factual associations with
Rank-One Model Editing (ROME). Being able to insert a new knowledge tuple tâˆ— = (s,r,oâˆ—) in
place of the current tupletc = (s,r,o c) with both generalization and speciï¬city would demonstrate
ï¬ne-grained understanding of the association-storage mechanisms.
3.1 Rank-One Model Editing: Viewing the Transformer MLP as an Associative Memory
We viewW (l)
proj as a linear associative memory (Kohonen, 1972; Anderson, 1972). This perspective
observes that any linear operation W can operate as a keyâ€“value store for a set of vector keys
K = [k1| k2| ... ] and corresponding vector values V = [v1| v2| ... ], by solving WK â‰ˆ V ,
whose squared error is minimized using the Moore-Penrose pseudoinverse:W =VK +. Bau et al.
(2020) observed that a new keyâ€“value pair (kâˆ—,vâˆ—) can be inserted optimally into the memory by
solving a constrained least-squares problem. In a convolutional network, Bau et al. solve this using
an optimization, but in a fully-connected layer, we can derive a closed form solution:
minimizeâˆ¥ Ë†WKâˆ’Vâˆ¥ such that Ë†Wkâˆ— =vâˆ— by setting Ë†W =W + Î›(Câˆ’1kâˆ—)T. (2)
HereW is the original matrix,C =KK T is a constant that we pre-cache by estimating the uncentered
covariance ofk from a sample of Wikipedia text (Appendix E.5), andÎ› = (vâˆ—âˆ’Wkâˆ—)/(Câˆ’1kâˆ—)Tkâˆ—
is a vector proportional to the residual error of the new keyâ€“value pair on the original memory matrix
(full derivation in Appendix A). Because of this simple algebraic structure, we can insert any fact
directly once (kâˆ—,vâˆ—) is computed. All that remains is to choose the appropriatekâˆ— andvâˆ—.
Step 1: Choosing kâˆ— to Select the Subject. Based on the decisive role of MLP inputs at the ï¬nal
subject token (Section 2), we shall choose inputs that represent the subject at its last token as the
lookup keykâˆ—. Speciï¬cally, we computekâˆ— by collecting activations: We pass text x containing
the subjects throughG; then at layerlâˆ— and last subject token indexi, we read the value after the
non-linearity inside the MLP (Figure 4d). Because the state will vary depending on tokens that
6Unrelated to keys and values in self-attention.
5
precedes in text, we setkâˆ— to an average value over a small set of texts ending with the subjects:
kâˆ— = 1
N
Nâˆ‘
j=1
k(xj +s), wherek(x) =Ïƒ
(
W (lâˆ—)
f c Î³(a(lâˆ—)
[x],i +h(lâˆ—âˆ’1)
[x],i )
)
. (3)
In practice, we samplexj by generating 50 random token sequences of length 2 to 10 usingG.
Step 2: Choosingvâˆ— to Recall the Fact. Next, we wish to choose some vector valuevâˆ— that encodes
the new relation (r,oâˆ—) as a property ofs. We setvâˆ— = argminzL(z), where the objectiveL(z) is:
1
N
Nâˆ‘
j=1
âˆ’ log PG(m(lâˆ— )
i :=z) [oâˆ—|xj +p ]
î´™ î´˜î´— î´š
(a) Maximizing oâˆ— probability
+ DKL
(
PG(m(lâˆ— )
iâ€² :=z) [x|pâ€²]
îµ¹îµ¹PG [x|pâ€²]
)
î´™ î´˜î´— î´š
(b) Controlling essence drift
. (4)
The ï¬rst term (Eqn. 4a) seeks a vectorz that, when substituted as the output of the MLP at the token
i at the end of the subject (notatedG(m(lâˆ—)
i :=z)), will cause the network to predict the target object
oâˆ— in response to the factual promptp. The second term (Eqn. 4b) minimizes the KL divergence of
predictions for the prompt pâ€² (of the form â€œ{subject} is aâ€) to the unchanged model, which helps
preserve the modelâ€™s understanding of the subjectâ€™s essence. To be clear, the optimization doesnot
directly alter model weights; it identiï¬es a vector representationvâˆ— that, when output at the targeted
MLP module, represents the new property (r,oâˆ—) for the subjects. Note that, similar tokâˆ— selection,
vâˆ— optimization also uses the random preï¬x textsxj to encourage robustness under differing contexts.
Step 3: Inserting the Fact. Once we have computed the pair ( kâˆ—, vâˆ—) to represent the full fact
(s,r,oâˆ—), we apply Eqn. 2, updating the MLP weightsW (l)
proj with a rank-one update that inserts the
new keyâ€“value association directly. For full implementation details, see Appendix E.5.
3.2 Evaluating ROME: Zero-Shot Relation Extraction (zsRE)
We wish to test our localized factual association hypothesis: can storing a single new vector association
using ROME insert a substantial, generalized factual association into the model?
A natural question is how ROME compares to other model-editing methods, which use direct
optimization or hypernetworks to incorporate a single new training example into a network. For
baselines, we examine Fine-Tuning (FT), which applies Adam with early stopping at one layer to
minimizeâˆ’ log P [oâˆ—|x]. Constrained Fine-Tuning (FT+L) (Zhu et al., 2020) additionally imposes a
parameter-spaceLâˆ norm constraint on weight changes. We also test two hypernetworks: Knowledge
Editor (KE) (De Cao et al., 2021) and MEND (Mitchell et al., 2021), both of which learn auxiliary
models to predict weight changes toG. Further details are described in Appendix E.
Table 1: zsRE Editing Results on GPT-2 XL.
Editor Efï¬cacy â†‘ Paraphrase â†‘ Speciï¬city â†‘
GPT-2 XL 22.2 ( Â± 0.5) 21.3 ( Â± 0.5) 24.2 ( Â± 0.5)
FT 99.6 ( Â± 0.1) 82.1 ( Â± 0.6) 23.2 ( Â± 0.5)
FT+L 92.3 ( Â± 0.4) 47.2 ( Â± 0.7) 23.4 ( Â± 0.5)
KE 65.5 ( Â± 0.6) 61.4 ( Â± 0.6) 24.9 ( Â± 0.5)
KE-zsRE 92.4 ( Â± 0.3) 90.0 ( Â± 0.3) 23.8 ( Â± 0.5)
MEND 75.9 ( Â± 0.5) 65.3 ( Â± 0.6) 24.1 ( Â± 0.5)
MEND-zsRE 99.4 ( Â± 0.1) 99.3 ( Â± 0.1) 24.1 ( Â± 0.5)
ROME 99.8 ( Â± 0.0) 88.1 ( Â± 0.5) 24.2 ( Â± 0.5)
We ï¬rst evaluate ROME on the Zero-Shot Re-
lation Extraction (zsRE) task used in Mitchell
et al. (2021) and De Cao et al. (2021). Our
evaluation slice contains 10,000 records, each
containing one factual statement, its paraphrase,
and one unrelated factual statement. â€œEfï¬cacyâ€
and â€œParaphraseâ€ measure post-edit accuracy
I
[
oâˆ— = argmaxoPGâ€² [o]
]
of the statement and
its paraphrase, respectively, while â€œSpeciï¬cityâ€
measures the edited modelâ€™s accuracy on an un-
related fact. Table 1 shows the results: ROME is
competitive with hypernetworks and ï¬ne-tuning
methods despite its simplicity. We ï¬nd that it
is not hard for ROME to insert an association that can be regurgitated by the model. Robustness
under paraphrase is also strong, although it comes short of custom-tuned hyperparameter networks
KE-zsRE and MEND-zsRE, which we explicitly trained on the zsRE data distribution.7 We ï¬nd that
zsREâ€™s speciï¬city score is not a sensitive measure of model damage, since these prompts are sampled
from a large space of possible facts, whereas bleedover is most likely to occur on related neighboring
subjects. Appendix C has additional experimental details.
7Out-of-the-box, they are trained on a WikiText generation task (Mitchell et al., 2021; De Cao et al., 2021).
6